version: "3"
services:
  namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    restart: always
    networks:
      - bigdata-network
    ports:
      - target: 9870
        published: 9870
        protocol: tcp
        mode: host
      - target: 9000
        published: 9000
        protocol: tcp
        mode: host
    env_file:
      - ./config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    deploy:
      endpoint_mode: dnsrr
      placement:
        constraints:
          - 'node.labels.nodename == node55'
  datanode:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    restart: always
    networks:
      - bigdata-network
    env_file:
      - ./config
    ports:
      - target: 1004
        published: 1004
        protocol: tcp
        mode: host
      - target: 1006
        published: 1006
        protocol: tcp
        mode: host
      - target: 9866
        published: 9864
        protocol: tcp
        mode: host
    deploy:
      endpoint_mode: dnsrr
      placement:
        constraints:
          - 'node.labels.nodename == node55'
  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    restart: always
    networks:
      - bigdata-network
    ports:
      - 8088:8088
    env_file:
      - ./config
    deploy:
      placement:
        constraints:
          - 'node.labels.nodename == node55'
  nodemanager:
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    restart: always
    networks:
      - bigdata-network
    env_file:
      - ./config
    deploy:
      placement:
        constraints:
          - 'node.labels.nodename == node55'

  spark-master:
    hostname: spark-master
    image: ${IMAGE_PREFIX}/spark-master:${IMAGE_TAG}
    container_name: spark-master
    restart: always
    networks:
      - bigdata-network
    ports:
      - "8080:8080"
      - "7077:7077"
      - "6066:6066"
    deploy:
      placement:
        constraints:
          - 'node.role == manager'
          - 'node.labels.nodename == node91'
  spark-slave:
    hostname: spark-worker
    image: ${IMAGE_PREFIX}spark-worker:${IMAGE_TAG}
    container_name: spark-worker
    restart: always
    networks:
      - bigdata-network
    ports:
      - "8781:8081"
    volumes:
      - /data/projects/:/data/projects/
    environment:
      SERVICE_PRECONDITION: "spark-master:7077"
      EGGROLL_HOME: "/data/projects/fate/eggroll"
      FATE_PROJECT_BASE: "/data/projects/fate"
      VIRTUAL_ENV: "/data/projects/python/venv"
      PYTHONPATH: "/data/projects/fate/:/data/projects/fate/eggroll/python:/data/projects/fate/fate/python:/data/projects/fate/fateflow/python:/data/projects/fate/python"
      PATH: "/data/projects/python/venv/bin:/opt/hadoop-3.2.3/bin:/opt/hive/bin:/opt/spark-3.1.3-bin-hadoop3.2/bin:/opt/hadoop-3.2.3/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
    deploy:
      placement:
        constraints:
          - 'node.role == manager'
          - 'node.labels.nodename == node91'
volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_datanode3:
  hadoop_historyserver:
  kerberos_db:
  kerberos_keytab:
    driver_opts:
      type: nfs
      o: addr=192.168.11.74,nfsvers=4,minorversion=0,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport
      device: :/home/centos/nfs_share/volume/bigdata/keytab
  hive_metastore:
  mysql_data:
  linkis_log:
  linkis_runtime:
networks:
  bigdata-network:
    external: true
    name: bigdata-network
